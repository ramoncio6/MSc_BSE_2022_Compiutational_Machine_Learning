{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "amino-custody",
   "metadata": {},
   "source": [
    "# <center><ins>Extended Test:</ins></center>\n",
    " # <center><ins> K-Nearest Neighbours (KNN) & Support Vector Machine (SVM)</ins></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saved-times",
   "metadata": {},
   "source": [
    "## <ins>Part 1: Set-Up</ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "round-liver",
   "metadata": {},
   "source": [
    " First of all, we import all the random packages and set a random seed to ensure the code is reproducible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "hindu-budapest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary packages\n",
    "import os,sys,inspect\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from math import floor, ceil\n",
    "from numpy import mean, std\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "from dfmapper import DataFrameMapper\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from category_encoders import TargetEncoder\n",
    "import category_encoders as ce\n",
    "import dateutil.parser\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "import plotly.express as px\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(3123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toxic-acoustic",
   "metadata": {},
   "source": [
    "The next task is to load our data. This includes: the 'mimic_train.csv' file with the train set -covariates and target variable included; the 'mimic_test_death.csv' file that provides the test set covariates; finally, we load the file 'mimic_diagnoses.csv' that includes the metadata. After the load, we represent the imported data in data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "rotary-shopper",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data and represent csv as dataframe\n",
    "data = pd.read_csv(\"/Users/Ramon/Documents/Estudio/BSE/Term_1/Computational_Machine_Learning/Tests/Test_2/mimic_train.csv\")\n",
    "data = pd.DataFrame(data)\n",
    "data.nunique()\n",
    "\n",
    "X_test0 = pd.read_csv(\"//Users/Ramon/Documents/Estudio/BSE/Term_1/Computational_Machine_Learning/Tests/Test_2/mimic_test_death.csv\")\n",
    "X_test0 = pd.DataFrame(X_test0)\n",
    "\n",
    "X_metadata = pd.read_csv(\"//Users/Ramon/Documents/Estudio/BSE/Term_1/Computational_Machine_Learning/Tests/Test_2/MIMIC_diagnoses.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "early-tuesday",
   "metadata": {},
   "source": [
    "A brief exploration of our datasets is performed to gain a preliminary intuition of our variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "greatest-welcome",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20885, 41)\n",
      "(5221, 39)\n",
      "Index(['HOSPITAL_EXPIRE_FLAG', 'subject_id', 'hadm_id', 'icustay_id',\n",
      "       'HeartRate_Min', 'HeartRate_Max', 'HeartRate_Mean', 'SysBP_Min',\n",
      "       'SysBP_Max', 'SysBP_Mean', 'DiasBP_Min', 'DiasBP_Max', 'DiasBP_Mean',\n",
      "       'MeanBP_Min', 'MeanBP_Max', 'MeanBP_Mean', 'RespRate_Min',\n",
      "       'RespRate_Max', 'RespRate_Mean', 'TempC_Min', 'TempC_Max', 'TempC_Mean',\n",
      "       'SpO2_Min', 'SpO2_Max', 'SpO2_Mean', 'Glucose_Min', 'Glucose_Max',\n",
      "       'Glucose_Mean', 'GENDER', 'DOB', 'ADMITTIME', 'Diff', 'ADMISSION_TYPE',\n",
      "       'INSURANCE', 'RELIGION', 'MARITAL_STATUS', 'ETHNICITY', 'DIAGNOSIS',\n",
      "       'ICD9_diagnosis', 'FIRST_CAREUNIT', 'LOS'],\n",
      "      dtype='object')\n",
      "0    18540\n",
      "1     2345\n",
      "Name: HOSPITAL_EXPIRE_FLAG, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Exploring Data Sets\n",
    "print(data.shape)\n",
    "print(X_test0.shape)\n",
    "print(data.columns)\n",
    "print(data[\"HOSPITAL_EXPIRE_FLAG\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "united-portuguese",
   "metadata": {},
   "source": [
    "## <ins>Part 2: Pre-Processing</ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "found-conservation",
   "metadata": {},
   "source": [
    "### A) Analyzing outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optional-milton",
   "metadata": {},
   "source": [
    "First of all, we contemplate the possibility of excluding outliers that are beyond 3 standard deviations from the mean. We only analyze some numerical variables in the training set for potencial outlier exclusion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "transsexual-senate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to remove outliers from a particular dataframe column\n",
    "def subset_by_iqr(df, column, whisker_width):\n",
    "    \"\"\"Remove outliers from a dataframe by column, including optional\n",
    "       whiskers, removing rows for which the column value are\n",
    "       less than Q1-1.5IQR or greater than Q3+1.5IQR.\n",
    "    Args:\n",
    "        df (`:obj:pd.DataFrame`): A pandas dataframe to subset\n",
    "        column (str): Name of the column to calculate the subset from.\n",
    "        whisker_width (float): Optional, loosen the IQR filter by a\n",
    "                               factor of `whisker_width` * IQR.\n",
    "    Returns:\n",
    "        (`:obj:pd.DataFrame`): Filtered dataframe\n",
    "    \"\"\"\n",
    "    # Calculate Q1, Q2 and IQR\n",
    "    q1 = df[column].quantile(0.25)\n",
    "    q3 = df[column].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    # Apply filter with respect to IQR, including optional whiskers\n",
    "    filter = (df[column] >= q1 - whisker_width*iqr) & (df[column] <= q3 + whisker_width*iqr)\n",
    "    return df.loc[filter]\n",
    "\n",
    "\n",
    "# Exclude outliers that are beyond 3 std from the mean for some numerical variables in training set\n",
    "data_outliers = subset_by_iqr(data, column='HeartRate_Min', whisker_width=3)\n",
    "data_outliers = subset_by_iqr(data_outliers, column='HeartRate_Max', whisker_width=3)\n",
    "\n",
    "data_outliers = subset_by_iqr(data_outliers, column='SysBP_Min', whisker_width=3)\n",
    "data_outliers = subset_by_iqr(data_outliers, column='SysBP_Max', whisker_width=3)\n",
    "\n",
    "data_outliers = subset_by_iqr(data_outliers, column='DiasBP_Min', whisker_width=3)\n",
    "data_outliers = subset_by_iqr(data_outliers, column='DiasBP_Max', whisker_width=3)\n",
    "\n",
    "data_outliers = subset_by_iqr(data_outliers, column='MeanBP_Min', whisker_width=3)\n",
    "data_outliers = subset_by_iqr(data_outliers, column='MeanBP_Max', whisker_width=3)\n",
    "\n",
    "data_outliers = subset_by_iqr(data_outliers, column='RespRate_Min', whisker_width=3)\n",
    "data_outliers = subset_by_iqr(data_outliers, column='RespRate_Max', whisker_width=3)\n",
    "\n",
    "data_outliers = subset_by_iqr(data_outliers, column='TempC_Min', whisker_width=3)\n",
    "data_outliers = subset_by_iqr(data_outliers, column='TempC_Max', whisker_width=3)\n",
    "\n",
    "data_outliers = subset_by_iqr(data_outliers, column='SpO2_Min', whisker_width=3)\n",
    "data_outliers = subset_by_iqr(data_outliers, column='SpO2_Max', whisker_width=3)\n",
    "\n",
    "data_outliers = subset_by_iqr(data_outliers, column='Glucose_Min', whisker_width=3)\n",
    "data_outliers = subset_by_iqr(data_outliers, column='Glucose_Max', whisker_width=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functioning-habitat",
   "metadata": {},
   "source": [
    "The impact on performance metrics provoked by the exclusion of outliers was assesed for different values of the whisker width. As outlier exclusion didn't have a significant impact on subsequent used performance metrics -it even worsen simulated predictions assesed by cross-validation- we opted to keep rare extreme values in our training set. This may seem sensible as extreme values in one variable may provide some valuable information regarding the probability of survival. In short, the evidence suggested it was convenient to keep outliers in our training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decimal-specific",
   "metadata": {},
   "source": [
    "### B) Working with Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "through-dining",
   "metadata": {},
   "source": [
    "The metadata provided information on the name of co-morbilities (ICD9_CODE) associated to a main diagnose for a subset of patients in the training set. Naturally, the type of co-morbility possibly adds explanatory power to our model attempting to predict the probability of survival. Our first step is to merge our metadata data frame with our target variable present in the training set. After this we target encoded the name of the co-morbilities: recall target encoding involves replacing the â€œfeatures with a blend of posterior probability of the target given particular categorical value and the prior probability of the target over all the training data\". In simple terms, we replace the categories considering the effect they might have on the target variable. After targeting encoding ICD9_CODE we computed the mean value of ICD9_CODE for each subject_id as a representative measure. Finally, we merged the encoded feature with our training set hence adding an aditional covariate. Note that unlike one hot encoding (extension), target encoding does not substantially increase dimensionality.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "modular-jacket",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/category_encoders/target_encoder.py:122: FutureWarning: Default parameter min_samples_leaf will change in version 2.6.See https://github.com/scikit-learn-contrib/category_encoders/issues/327\n",
      "  warnings.warn(\"Default parameter min_samples_leaf will change in version 2.6.\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/category_encoders/target_encoder.py:127: FutureWarning: Default parameter smoothing will change in version 2.6.See https://github.com/scikit-learn-contrib/category_encoders/issues/327\n",
      "  warnings.warn(\"Default parameter smoothing will change in version 2.6.\"\n"
     ]
    }
   ],
   "source": [
    "# Target encode variable ICD9_CODE from metadata\n",
    "\n",
    "## Rename metadata columns to ensure compatibility with training set\n",
    "X_metadata = X_metadata.rename(columns={'SUBJECT_ID':'subject_id'})\n",
    "X_metadata = X_metadata.rename(columns={'HADM_ID':'hadm_id'})\n",
    "\n",
    "# Merge the relevant columns from training set with metadata using subject_id as our main key\n",
    "cols_y = data[['subject_id','HOSPITAL_EXPIRE_FLAG']]\n",
    "X_metadataA = X_metadata.merge(cols_y,how='inner',on='subject_id')\n",
    "\n",
    "# Choose encoder, fit and transform our mutated metadata dataframe\n",
    "encoder = TargetEncoder()\n",
    "encoded_ICD9 = encoder.fit_transform(X_metadataA[['ICD9_CODE']],X_metadataA[['HOSPITAL_EXPIRE_FLAG']])\n",
    "\n",
    "# Concatenate our mutated metadata dataframe with encoded feature \n",
    "X_metadataB = pd.concat([X_metadataA,encoded_ICD9],axis=1)\n",
    "\n",
    "# Group mutated metadata frame with encoded feature by subject_id and hadm_id, \n",
    "# and calculate mean of the target encoded variable to add as a new feature in training set\n",
    "X_metadataC = X_metadataB.groupby(['subject_id','hadm_id'],as_index=False)['ICD9_CODE'].mean()\n",
    "\n",
    "# Add target encoded variable ICD9_CODE to training set\n",
    "data0 = data.merge(X_metadataC,how='left',on=['subject_id','hadm_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intellectual-water",
   "metadata": {},
   "source": [
    "The analogous process as the preceeding one was performed for the test set. Nonetheless, instead of using the test set to fit and transform our encoder we use the training set data to impute variable ICD9_CODE in our test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "together-harvard",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/category_encoders/target_encoder.py:122: FutureWarning: Default parameter min_samples_leaf will change in version 2.6.See https://github.com/scikit-learn-contrib/category_encoders/issues/327\n",
      "  warnings.warn(\"Default parameter min_samples_leaf will change in version 2.6.\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/category_encoders/target_encoder.py:127: FutureWarning: Default parameter smoothing will change in version 2.6.See https://github.com/scikit-learn-contrib/category_encoders/issues/327\n",
      "  warnings.warn(\"Default parameter smoothing will change in version 2.6.\"\n"
     ]
    }
   ],
   "source": [
    "# Select subject_id column in test set and merge feature with metadata dataframe using subject_id as main key\n",
    "cols_y_T = X_test0[['subject_id']]\n",
    "X_metadata1 = X_metadata.merge(cols_y_T,how='inner',on='subject_id')\n",
    "\n",
    "# Choose encoder, fit it in the training set and transform the test set \n",
    "encoder = TargetEncoder()\n",
    "encoded_ICD9 = encoder.fit(X_metadataA[['ICD9_CODE']],X_metadataA[['HOSPITAL_EXPIRE_FLAG']])\n",
    "encoded_ICD9_T = encoder.transform(X_metadata1[['ICD9_CODE']])\n",
    "\n",
    "# Concatenate our metadata for test set with target encoded feature ICD9_CODE\n",
    "X_metadata2 = pd.concat([X_metadata1,encoded_ICD9_T],axis=1)\n",
    "\n",
    "# Select the relevant columns from metadata: subject_id, hadm_id and ICD9_CODE\n",
    "X_metadata3 = X_metadata2.iloc[:,[0,1,4]]\n",
    "\n",
    "# Group mutated metadata frame with encoded feature by subject_id and hadm_id, \n",
    "# and calculate mean of the target encoded variable to add as a new feature in test set\n",
    "X_metadata4 = X_metadata3.groupby(['subject_id','hadm_id'],as_index=False)['ICD9_CODE'].mean()\n",
    "\n",
    "# Add target encoded variable ICD9_CODE to test set\n",
    "X_testA = X_test0.merge(X_metadata4,how='left',on=['subject_id','hadm_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "restricted-territory",
   "metadata": {},
   "source": [
    "Another potentially useful information implicitely present in the metadata is the number of co-morbilities. We would like to include this feature as an explanatory variable both on the training set and test set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "cleared-music",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure our metadata column names have been renamed appropiately for compatibility with training set\n",
    "X_metadata = X_metadata.rename(columns={'SUBJECT_ID':'subject_id'})\n",
    "X_metadata = X_metadata.rename(columns={'HADM_ID':'hadm_id'})\n",
    "\n",
    "# Group our metadata by subject_id and hadm_id and count the number of comorbilities indicated by the column SEQ_NUM\n",
    "X_metadata2 = X_metadata.groupby(['subject_id','hadm_id'],as_index=False)['SEQ_NUM'].count()\n",
    "\n",
    "# Merge our newly created variable with training set using subject_id and hadm_id as keys\n",
    "data1 = data0.merge(X_metadata2,how='left',on=['subject_id','hadm_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "known-prompt",
   "metadata": {},
   "source": [
    "Analogously, we add the newly created explanatory variable to the corresponding patients in the test set, using -as in the trainig set merge- subject_id and hadm_id as keys. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "associate-truth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge test set with metadata to conform the new test set using subject_id and hadm_id as keys\n",
    "X_testB = X_testA.merge(X_metadata2,how='left',on=['subject_id','hadm_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "damaged-controversy",
   "metadata": {},
   "source": [
    "### C) Creating Age variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "least-peninsula",
   "metadata": {},
   "source": [
    "Despite the age of each patient is not explicitely present in the dataset, it may be infered. We have the admision time and the date of birth of almost every patient. Therefore, by performing a simple subtraction we can obtain the estimated age of most of the patients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "indirect-exception",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Select appropiate columns from the training set to find date difference\n",
    "data_age = data1[['DOB', 'ADMITTIME']]\n",
    "\n",
    "# We convert the relevant columns to standard date time represent them in a dataframe \n",
    "data_age_dob= pd.to_datetime(data_age['DOB'])\n",
    "data_age_adm= pd.to_datetime(data_age['ADMITTIME'])\n",
    "data_age_dob = pd.DataFrame(data_age_dob)\n",
    "data_age_adm = pd.DataFrame(data_age_adm)\n",
    "\n",
    "# Extract the year of each of the relevant columns\n",
    "data_age_dob['Age'] = pd.DatetimeIndex(data_age_dob.iloc[:,0]).year\n",
    "data_age_adm['Age'] = pd.DatetimeIndex(data_age_adm.iloc[:,0]).year\n",
    "\n",
    "# Perform the subtraction to find the age and load it into a dataframe\n",
    "data_diff = data_age_adm['Age']-data_age_dob['Age']\n",
    "data_diff=pd.DataFrame(data_diff )\n",
    "\n",
    "# Represent the Age in a histogram\n",
    "plt.xlabel(\"Age\", fontsize=16)\n",
    "plt.ylabel(\"Frequency\", fontsize=16)\n",
    "plt.hist(data_diff,bins=100,range=[0, 350],label='AGE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enabling-passport",
   "metadata": {},
   "source": [
    "As we can witness from the histogram, there are some values above 300 that are not plausible. After some online investigation we discovered that \"age for patients older than eighty-nine is masked as 300 in MIMIC- III for privacy reasons\". Therefore, we firstly found the difference between the implausible ages and 89 and secondly subtracted to each implausible value the obtained difference in the preceeding step. The result is a sensible histogram for age distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "postal-boating",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Filter for ages below 100 and obtain the maximum value to certify its 89\n",
    "below_100 = data_diff[data_diff['Age']<100]\n",
    "below_100.max()\n",
    "\n",
    "# Filter for ages above 100 and obtain the min which should be 300 (histogram)\n",
    "above_100 = data_diff[data_diff['Age']>100]\n",
    "above_100.min()\n",
    "\n",
    "# Subtract to each age above 100 the difference between 300 and 89 \n",
    "data_diff['Age'] = np.where(data_diff['Age'] >100 , data_diff['Age'] - (300-89), data_diff['Age'])\n",
    "\n",
    "# plot histogram with appropiate labels to certify a sensible age distribution was obtained\n",
    "plt.xlabel(\"Age\", fontsize=16)\n",
    "plt.ylabel(\"Frequency\", fontsize=16)\n",
    "plt.hist(data_diff,bins=100,range=[0, 350])\n",
    "plt.show()\n",
    "\n",
    "# Merge the new age column in our training set\n",
    "data2= pd.concat([data1,data_diff],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colored-galaxy",
   "metadata": {},
   "source": [
    "The analogous procedure was performed in the test set to obtain the corresponding age of each patient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "portuguese-nutrition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Select appropiate columns from the test set to find date difference\n",
    "data_age1 = X_testB[['DOB', 'ADMITTIME']]\n",
    "\n",
    "# We convert the relevant columns to standard date time and represent them in a dataframe \n",
    "data_age_dob1= pd.to_datetime(data_age1['DOB'])\n",
    "data_age_adm1= pd.to_datetime(data_age1['ADMITTIME'])\n",
    "data_age_dob1 = pd.DataFrame(data_age_dob1)\n",
    "data_age_adm1 = pd.DataFrame(data_age_adm1)\n",
    "\n",
    "# Extract the year of each of the relevant columns\n",
    "data_age_dob1['Age'] = pd.DatetimeIndex(data_age_dob1.iloc[:,0]).year\n",
    "data_age_adm1['Age'] = pd.DatetimeIndex(data_age_adm1.iloc[:,0]).year\n",
    "\n",
    "# Perform the subtraction to find the age and load it into a dataframe\n",
    "data_diff1 = data_age_adm1['Age']-data_age_dob1['Age']\n",
    "data_diff1=pd.DataFrame(data_diff1)\n",
    "\n",
    "# Represent the Age in a histogram\n",
    "plt.xlabel(\"Age\", fontsize=16)\n",
    "plt.ylabel(\"Frequency\", fontsize=16)\n",
    "plt.hist(data_diff1,bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "catholic-event",
   "metadata": {},
   "source": [
    "Given the same problem we had with age in the training set arises in the test set, we adopt the same strategy to deal with the inconsistencies. As we witness in the new histogram, the age distribution is now adequately represented. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "continent-whole",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Filter for ages below 100 and obtain the maximum value to certify its 89\n",
    "below_100_ = data_diff1[data_diff1['Age']<100]\n",
    "below_100_.max()\n",
    "\n",
    "# Filter for ages above 100 and obtain the min which should be from histogram 300\n",
    "above_100_ = data_diff1[data_diff1['Age']>100]\n",
    "above_100_.min()\n",
    "\n",
    "# Subtract to each age above 100 the difference between the minimum age above 100 and 89\n",
    "data_diff1['Age'] = np.where(data_diff1['Age'] >100 , data_diff1['Age'] - (300-89), data_diff1['Age'])\n",
    "data_diff1.max()\n",
    "\n",
    "# Plot histogram with appropiate labels to certify a sensible age distribution was obtained\n",
    "plt.xlabel(\"Age\", fontsize=16)\n",
    "plt.ylabel(\"Frequency\", fontsize=16)\n",
    "plt.hist(data_diff1,bins=100,range=[0, 350])\n",
    "plt.show()\n",
    "\n",
    "# Merge the new age column in our test set\n",
    "X_testC= pd.concat([X_testB,data_diff1],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-clark",
   "metadata": {},
   "source": [
    "### D) Train-Set Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "everyday-leader",
   "metadata": {},
   "source": [
    "Furtermore, we explicitely separate our training set covariates, our training set target variable and our test set covariates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "backed-command",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select explanatory variables from traing data\n",
    "X_train = data2.drop([\"HOSPITAL_EXPIRE_FLAG\"],axis=1)\n",
    "\n",
    "# Select target variable from traing data\n",
    "y_train = data2[[\"HOSPITAL_EXPIRE_FLAG\"]]\n",
    "\n",
    "# Load test set covariates in a data frame\n",
    "X_test = pd.DataFrame(X_testC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historic-transsexual",
   "metadata": {},
   "source": [
    "Because not all of the variables in the training set are present in the test set, we drop those covariates that are not present in both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "absent-infrastructure",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop variables in train set that are not in test set\n",
    "col_not_to_drop = X_test.columns\n",
    "X_train = X_train[col_not_to_drop]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modular-ready",
   "metadata": {},
   "source": [
    "A preliminary treatment of missing values involves dropping those columns that have 15% or more missing values as the information cost associated to imputing them may outweight their explicative power. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "posted-carpet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check features that have a'lot of missing values\n",
    "X_train.isna().sum()\n",
    "\n",
    "# Percentage of null in every column\n",
    "null_percentage = X_train.isnull().sum()/X_train.shape[0]*100\n",
    "\n",
    "# Drop columns having more than 15% null\n",
    "col_to_drop = null_percentage[null_percentage>15].keys()\n",
    "X_train1 = X_train.drop(col_to_drop, axis=1)\n",
    "\n",
    "# Update test set with columns dropped in training set\n",
    "X_test1 = X_test.drop(col_to_drop,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attached-computer",
   "metadata": {},
   "source": [
    "As no columns have more that 15% of missing values, the X training set wasn't affected. Concomitantly, no variables were dropped in the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-participation",
   "metadata": {},
   "source": [
    "### E) Target encoding & scalarizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-fancy",
   "metadata": {},
   "source": [
    "In the following section we will proceed to target encode the categorical features and scalarize numerical ones. We will work with training set and test set simultaneously. Note the scalarization and target encoding of the test set will be done using only the training set data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forty-publicity",
   "metadata": {},
   "source": [
    "We first start by generating the following data frames: X train categorical, X train numerical, X test categorical and X test numerical. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "experienced-sight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distinguish between data types of numerical columns (int,float) and categorical (object) on training set\n",
    "numeric_columns=list(X_train1.select_dtypes(['int64','float64']).columns)\n",
    "categorical_columns=list(X_train1.select_dtypes('object').columns)\n",
    "\n",
    "# Filter separating categorical columns and numerical columns in training set\n",
    "X_train_cat = X_train1.filter(categorical_columns, axis=1)\n",
    "X_train_num = X_train1.filter(numeric_columns, axis=1)\n",
    "\n",
    "# Distinguish between data types of numerical columns (int,float) and categorical (object) on test set\n",
    "numeric_columns=list(X_test1.select_dtypes(['int64','float64']).columns)\n",
    "categorical_columns=list(X_test1.select_dtypes('object').columns)\n",
    "\n",
    "# Filter separating categorical columns and numerical columns in test set\n",
    "X_test_cat = X_test1.filter(categorical_columns, axis=1)\n",
    "X_test_num = X_test1.filter(numeric_columns, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minimal-checkout",
   "metadata": {},
   "source": [
    "We now proceed to scalarize numerical columns of training set and test set (based on training set metrics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "junior-excellence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define scaler\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "# Fit scaler to training set\n",
    "scaler.fit(X_train_num)\n",
    "\n",
    "# Transform test set and training set using preceeding scalerizer fitted in train set\n",
    "X_train_num1 = scaler.transform(X_train_num)\n",
    "X_test_num1 = scaler.transform(X_test_num)\n",
    "\n",
    "# Convert the scalarized numerical columns of train/test set to data frame\n",
    "X_train_num = pd.DataFrame(X_train_num1,columns=X_train_num.columns)\n",
    "X_test_num = pd.DataFrame(X_test_num1,columns=X_test_num.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-organic",
   "metadata": {},
   "source": [
    "Now we proceed to target encode categorical variables both in the train set and the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "handy-ukraine",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/category_encoders/target_encoder.py:122: FutureWarning: Default parameter min_samples_leaf will change in version 2.6.See https://github.com/scikit-learn-contrib/category_encoders/issues/327\n",
      "  warnings.warn(\"Default parameter min_samples_leaf will change in version 2.6.\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/category_encoders/target_encoder.py:127: FutureWarning: Default parameter smoothing will change in version 2.6.See https://github.com/scikit-learn-contrib/category_encoders/issues/327\n",
      "  warnings.warn(\"Default parameter smoothing will change in version 2.6.\"\n"
     ]
    }
   ],
   "source": [
    "# Define target encoder\n",
    "encoder = TargetEncoder()\n",
    "\n",
    "# Fit and transform training set using target encoding\n",
    "X_train_cat_enc = encoder.fit_transform(X_train_cat,y_train)\n",
    "\n",
    "# Represent encoded categorical variables of training set in a data frame\n",
    "X_train_cat_enc = pd.DataFrame(X_train_cat_enc)\n",
    "\n",
    "# Target encode categorical variables in test set using train set data\n",
    "X_test_cat_enc = encoder.transform(X_test_cat)\n",
    "\n",
    "# Represent encoded categorical variables of test set in a data frame\n",
    "X_test_cat_enc = pd.DataFrame(X_test_cat_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appointed-paste",
   "metadata": {},
   "source": [
    "We finally combine back numerical and categorical columns on both training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "affected-gentleman",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate scalarized numerical variables and encoded categorical variables in one data fram (train set)\n",
    "X_train2 = pd.concat([X_train_num, X_train_cat_enc], axis=1)\n",
    "\n",
    "# Concatenate scalarized numerical variables and encoded categorical variables in one data fram (test set)\n",
    "X_test2 = pd.concat([X_test_num, X_test_cat_enc], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integral-grounds",
   "metadata": {},
   "source": [
    "### F) Imputing missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seventh-tribe",
   "metadata": {},
   "source": [
    "How to impute missing values is one of the most importante tasks in the pre processing of the data. We will use k nearest neighbors through KNNImputer in order to impute missing values of a patient. The strategy is to replace a missing value of a specific feature of a given patient A using the records of the set of patients that resemble the most patient A in other recorded features. In other words, imputing missing values by similarity. \n",
    "The natural question that comes up is how many neighbors or similar patients we should consider to fill our missing values. To determine this we assumed a potential standard Knn model (number of neighbors, weights and algorithm exogenous) and fed it with different possible values of number of neighbors. This procedure will suggest the optimal number of neighbors to use to impute our missing values. Cross validation is used to asses predictive capacity of the model for the different possible neighbors and criteria of optimality is provided by area under the curve (AUC) performance metric. \n",
    "The following pipeline serves this purpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closing-generator",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   4 | elapsed:   37.9s remaining:   37.9s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   38.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">6 0.932 (0.003)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   4 | elapsed:   33.6s remaining:   33.6s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   35.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">10 0.931 (0.003)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   4 | elapsed:   49.7s remaining:   49.7s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   51.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">20 0.930 (0.004)\n"
     ]
    }
   ],
   "source": [
    "# Create an empty list to store results\n",
    "results = list()\n",
    "\n",
    "# Choose the potential nearest neighbor values \n",
    "strategies = [str(i) for i in [6,10,20,35,50]]\n",
    "for s in strategies:\n",
    "    # Create the modeling pipeline using standard Knn model \n",
    "    pipeline = Pipeline(steps=[('i', KNNImputer(n_neighbors=int(s))), ('m',\n",
    "               KNeighborsClassifier(weights='distance',algorithm='auto',metric='manhattan',n_neighbors=300))])\n",
    "    # Evaluate the model's predictive capacity thprugh cross validation using roc_auc criteria\n",
    "    cv = RepeatedStratifiedKFold(n_splits=2, n_repeats=2, random_state=1)\n",
    "    scores = cross_val_score(pipeline, X_train2, y_train, scoring='roc_auc', cv=cv, n_jobs=-1,verbose=True)\n",
    "    # Store results\n",
    "    results.append(scores)\n",
    "    print('>%s %.3f (%.3f)' % (s, np.mean(scores), np.std(scores)))\n",
    "\n",
    "# Plot box plot \n",
    "plt.boxplot(results, labels=strategies, showmeans=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infectious-combining",
   "metadata": {},
   "source": [
    "Note the boxplot represents the accurracy for the different proposed number of neighbors. 6 neighbors is the optimal number of neighbors to include in our knn imputer. Now we can proceed to replace the missing values via KNNImputer in both our training set and test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facial-privilege",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Knn imputer with the number of neighbors to use\n",
    "imputer = KNNImputer(n_neighbors=6)\n",
    "\n",
    "# Fit and transform the training data using imputer\n",
    "X_train3 = imputer.fit_transform(X_train2)\n",
    "\n",
    "# Represent the imputed training data in a data frame\n",
    "X_train3 = pd.DataFrame(X_train3,columns=X_train2.columns)\n",
    "\n",
    "# Verify missing values were imputed appopiately\n",
    "X_train3.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "copyrighted-healing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Knn imputer with the number of neighbors to use\n",
    "imputer = KNNImputer(n_neighbors=6)\n",
    "\n",
    "# Fit test data using imputer\n",
    "imputer.fit(X_train2)\n",
    "\n",
    "# Transform the testdata using imputer\n",
    "X_test3 = imputer.transform(X_test2)\n",
    "\n",
    "# Represent the imputed test data in a data frame\n",
    "X_test3 = pd.DataFrame(X_test3,columns=X_test2.columns)\n",
    "\n",
    "# Verify missing values were imputed appopiately\n",
    "X_test3.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consistent-spectrum",
   "metadata": {},
   "source": [
    "### G) Variable Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generous-visiting",
   "metadata": {},
   "source": [
    "The posterior step in our analysis is to select those variables that we anticipate have some incidence on the patients probability of survival and dropping the rest. Recall that adding variables that have no explanatory power whatsover generates noise that erodes the capacity of our model to capture the underlying pattern in the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cubic-emperor",
   "metadata": {},
   "source": [
    "We first find the correlation betweeen each feature and our target future to explore which variables might be relevant and undergo a process of variable selection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developing-cruise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate X train set with y train set to investigate correlations\n",
    "data3 = pd.concat([X_train3,y_train],axis=1)\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = data3.corr()\n",
    "\n",
    "# Represent it in a data frame and select column involving target variable\n",
    "corr = pd.DataFrame(corr[['HOSPITAL_EXPIRE_FLAG']])\n",
    "corr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "double-palace",
   "metadata": {},
   "source": [
    "Naturally, those columns that serve the purpose of identifying the patient (subject_id & hadm_id) will have no impact whatsoever in our target variable. We also drop date of birth (DOB) and admission time (ADMITTIME) that were already used to estimate the age and by themselves add no explanatory power. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "located-zealand",
   "metadata": {},
   "source": [
    "Additional features such as religion,gender and marital status and are candidates to be discarded as we suspect they are neutral with respect to the survival probability (some trials were performed with inclusion of theses variables and performance did not improve). Moreover, the particularly low correlation presents aditional evidence to support model exclusion. \n",
    "Furthermore, the mean of each of the medical categories (heart rate, sys, dias, etc) measured were dropped: not only they are composed of variables already present in the data (max and min) that precisely capture the presence of lower or upper deviations, but we conjecture that it is the presence of extreme values in a given category what impacts our target variable. In other words, the mean value of each medical category adds no relevant information. This is certified by our correlation column given the correlation of the mean value of a medical category is always below the correlation with either the minimum value or the maximimum value.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantitative-schedule",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features from training set\n",
    "X_train3.columns\n",
    "\n",
    "# Drop suspected irrelevant columns\n",
    "X_train4= X_train3.drop(['subject_id','hadm_id','icustay_id','DOB','ADMITTIME','RELIGION','Diff','HeartRate_Mean',\n",
    "                         'SysBP_Mean','DiasBP_Mean','MeanBP_Mean','RespRate_Mean','TempC_Mean','SpO2_Mean',\n",
    "                          'Glucose_Mean','GENDER','MARITAL_STATUS'],axis=1)\n",
    "X_train4.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-insight",
   "metadata": {},
   "source": [
    "The variables kept we speculate most likely affect our target variable. Those include the minimum and maximum values of each medical category such as heart rate, Sys, Dias, MeanBP, respiratory rate and temperature. Otherfeatures such as admission type, medical insurance, diagnosis, taget encoded ICD9_CODE, the first careunit. the number of co-morbilities and the age most likely play a role in explaining the probability of survival. Ethnicity was included because after some trials the explanatory power marginally increased. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suffering-failure",
   "metadata": {},
   "source": [
    "The concomitant exclusion was performed in the X test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moral-auditor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save icustay_id before dropping to merge identify patients once predictions are made\n",
    "icustay_id=X_test3[\"icustay_id\"]\n",
    "\n",
    "# Drop irrelevant columns in the X test set\n",
    "X_test4 = X_test3.drop(['subject_id','hadm_id','icustay_id','DOB','ADMITTIME','RELIGION','Diff','HeartRate_Mean',\n",
    "                         'SysBP_Mean','DiasBP_Mean','MeanBP_Mean','RespRate_Mean','TempC_Mean','SpO2_Mean',\n",
    "                          'Glucose_Mean','GENDER','MARITAL_STATUS'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinated-conspiracy",
   "metadata": {},
   "source": [
    "## <ins>Part 3: Knn Model</ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "horizontal-savings",
   "metadata": {},
   "source": [
    "The K Nearest Neighbors model is a supervised leraning classifier which uses proximity -computed by a measure of distance- to classify or predict a given data point. In a classification task, a given observation is labeled considering the relative proximity of the k nearest data point, where k is a hyperparameter to be tuned. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architectural-edward",
   "metadata": {},
   "source": [
    " The hyperparameters are exogenous parameters that are not determined within the model. Even though we will undergo a tuning process to select most of them, we will already fix two of the hyperparameter when instatiating the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southern-infrared",
   "metadata": {},
   "source": [
    "First of all we instatiate our KNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "potential-scheme",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instatiate KNN\n",
    "MyKNN = KNeighborsClassifier(weights='distance',metric='minkowski')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defined-initial",
   "metadata": {},
   "source": [
    "The \"weights\" argument refers to the criteria used to assign a relative weight to the neighboring data points used to classify a particular observation. We prefer to use a distance metric over uniformly weighting all surrounding data points given we know from scratch that we want closer data points to have more weight than those that are further apart. Moreover, we will define the metric to measure distances to the minkowski one, given both the euclidean distance (p=2) and the manhattan distance (p=1) are particular cases of the minkowski distance (the cosine distance is another distance metric that could also be considered). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nasty-alabama",
   "metadata": {},
   "source": [
    "A grid search is a tuning technique used to determine the optimal values of hyperparameters. Optimality is evaluated by a given performance metric. In essence, we train models with different combinations of hyperparameters to discover which performs the best given a performance metric.\n",
    "Moreover, we define our model, the parameter grid, performance criteria, number of cross validations folds and finally fit the model to the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungarian-commission",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid Values/search\n",
    "#grid_values = {'n_neighbors':[10,30,50,100,150,200,350,368,400,500,700,1000,2000],'p':[1,1.5,2,2.5,3],\n",
    "#               'algorithm':['auto', 'ball_tree', 'kd_tree', 'brute']}\n",
    "\n",
    "\n",
    "#Define model,paramteter grid, number cross validations folds andscoring criteria KNN\n",
    "#grid_knn_acc = GridSearchCV(MyKNN, param_grid = grid_values, scoring = 'roc_auc', cv=20,n_jobs=-1,verbose=3)\n",
    "\n",
    "#Fit KNN on training data\n",
    "#grid_knn_acc.fit(X_train4, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statistical-luther",
   "metadata": {},
   "source": [
    "The essential hyperparameter is the number of near neighbors (k) we should consider to establish proximity, or in other words, how many neighbors will be checked to determine the classification of a given data point. Several values of k were tested ranging from 10 to 2000.\n",
    "\n",
    "Another hyperparamter to consider is the metric distance used to measure proximity between observations: in this case, when p=1 the minkowski distance is equals the manhattan distance and when p=2 it equals the euclidean distance. These are te most known distance measures but intermediates and more extreme values of p are also possible. Usually manhattan distance is preferred over the euclidean distance in high dimnesionality settings. \n",
    "\n",
    "The algorithm hyperparameter defines the algorithm used to compute the nearest neighbors. Auto will attempt to decide the most appropriate algorithm based on the values. kd_tree partitions the feature space in cubes and instead of calculating the distance of a new observation with every training point it only calculates the distance from the node. Ball tree does something similar but partitions the sample space in balls. Brute force simply calculates the distance from each query feature vector to each reference feature vector in the training data set.\n",
    "\n",
    "Cross validation is a re-sampling technique that partitions the data in k folds, uses k-1 folds to train the model and the remaining k fold to test its performance. It repeats the process for each of the k folds. We use cross validation as an artifitial simulation to judge how our different models will generalize or perform when faced with new unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hourly-wrong",
   "metadata": {},
   "source": [
    "Finally, we select the model that attained the highest accurracy given by the roc curve and the are under the curve. The ROC curve is a probability curve that plots the true positive rate vs the false positive rate. It represents the predictive ability of a binary classifier for all possible probability thresholds. \n",
    "The area under the ROC curve (AUC) is a measure of how much a model can distinguish between different classes when using different probability thresholds. The higher the AUC the better. We choose this criteria to evaluate our model performance.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acute-oxford",
   "metadata": {},
   "source": [
    "We construct a table to observe the mean accurracy and standard deviation of all possible model combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consistent-slope",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table obtaining mean and std of all possible models\n",
    "#res = pd.DataFrame(grid_knn_acc.cv_results_)\n",
    "#res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olympic-teens",
   "metadata": {},
   "source": [
    "The following output allows to acknowledge the definitive best performing model and its accurracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manufactured-arrangement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display best score and paramter combination attained by best model\n",
    "#grid_knn_acc.best_score_\n",
    "#grid_knn_acc.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civic-bailey",
   "metadata": {},
   "source": [
    "We then train our final model using the best estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interracial-schema",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model with tuned hyperparameters\n",
    "my_best_KNN_model = KNeighborsClassifier(n_neighbors=368, weights='distance',metric='manhattan')\n",
    "# Fit final model\n",
    "my_best_KNN_model.fit(X_train4, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legitimate-thriller",
   "metadata": {},
   "source": [
    "Furthermore, we predict probability of each test set observation with the best model fitted in the training set. (Note that knn has to estimate the probability of a given observation by making an \"forced\" logitic transformation). \n",
    "We then represent our predictions in a dataframe along icustay_id to identify the patient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reflected-chapel",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict Probability for each patient on test set\n",
    "y_pred_prob_knn = my_best_KNN_model.predict_proba(X_test4)\n",
    "\n",
    "# Represent it in a data frame\n",
    "y_pred_prob_knn = pd.DataFrame(y_pred_prob_knn)\n",
    "\n",
    "# Choose column on probability of dying\n",
    "y_pred_prob_knn_k = y_pred_prob_knn.iloc[:,1]\n",
    "\n",
    "# Final data frame with icustay_id identifying patient and corresponding prediction\n",
    "test_predictions_submit_knn = pd.DataFrame({\"icustay_id\": icustay_id, \"Hospital_Expire_Flag\": y_pred_prob_knn_k})\n",
    "\n",
    "# Covert predictions to csv and save\n",
    "# test_predictions_submit_knn.to_csv(\"predictionKNN.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accredited-handle",
   "metadata": {},
   "source": [
    "Finally, we can now compare our predicted probabilities for the test set patients with the real \"unseen\" outcome of each patient (death or survival)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-kinase",
   "metadata": {},
   "source": [
    "## <ins>Part 3: SVM Model</ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executive-humanity",
   "metadata": {},
   "source": [
    "Support Vector Machines for classification is a supervised learning algorithm that attempts to construct hyperplane in a N dimensional space to separate the data points into different classes. The objective of SVM (classification task) is to find a plane that has the maximum distance between classification points of different classes (Margin). A bigger margin implies more model robustness: a higher confidence in classifying future data points. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominant-terrain",
   "metadata": {},
   "source": [
    "Before instatiating our model we witness that the number of observations of one of the classes in our target variable is much smaller that the other. Therefore we need account for what is known as class imbalance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-theta",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-citizenship",
   "metadata": {},
   "source": [
    "We instatiate our SVM model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annual-means",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instatiate SVM model\n",
    "MySvc = SVC(class_weight='balanced',probability=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silver-carol",
   "metadata": {},
   "source": [
    "Note that class weight is set to balanced to compensate for the minority class in our target variable. Essentially, SVM deals with class imbalance by assigning higher misclassification penalties to training instances of the minority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacterial-definition",
   "metadata": {},
   "source": [
    "We now set grid serach values to train models with different combinations of hyperparameters to discover which performs the best given a performance metric. Moreover, we define our model, the parameter grid, performance criteria, number of cross validations folds and finally fit the model to the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabulous-orleans",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set grid values\n",
    "#grid_values = {'C': [0.5,1,2,5,10],'gamma':[0.05,0.1,0.5,1],'kernel':['linear', 'poly', 'rbf', 'sigmoid']}\n",
    "\n",
    "#Define model,paramteter grid,number cross validations folds and scoring criteria SVM\n",
    "#grid_svc_acc = GridSearchCV(MySvc, param_grid = grid_values,scoring = 'roc_auc', cv=20,verbose=3,n_jobs=-1)\n",
    "\n",
    "#Fit SVM on training data\n",
    "#grid_svc_acc.fit(X_train4, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understood-sentence",
   "metadata": {},
   "source": [
    "The hyperparameter C (in sklearn 1/C) is a regularization or cost paramter that represents how much we want to avoid misclassifying each training example. A small C implies a wider margin at the cost of misclassifications. On the other hand, a large C results in a small margin with less misclassifcations. Basically, C governs the tradeoff between the size of the margin or model robustness and having misclassification errors.\n",
    "\n",
    "Non linearly separable data can be turn out to be linearly separable when projected into a higher dimensional space. The virtue of SVM is that we can compute the distance in the high dimensional space without going there. In other words, the distances in high dimensional space can be written as a function of the low dimensional space. The kernel function computes the inner product -a measure of distance- in the transformed space. Our grid search involves four possible kernels: linear, polynomial, radial basis function and sigmoid. \n",
    "\n",
    "The gamma hyperparameter is kernel coefficent for â€˜rbfâ€™ and â€˜polyâ€™. In the polynomial kernel gamma is a coefficient that multiplies $x_i.x_j$. In the rbf kernel gamma it multiplies $||x_i - x_j||$. Intuitevely, gamma is a measure of how far the influence of asingle data point reaches. For a small gamma, the model behaves almost as a linear SVM without kernel and a large gamma implies it is heavily impacted by support vectors (may result in overfitting). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resistant-antique",
   "metadata": {},
   "source": [
    "Finally, we select the model that attained the highest accurracy given by the roc curve and the are under the curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "celtic-format",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display mean accurracy and std of all the possible set of models\n",
    "#res1 = pd.DataFrame(grid_svc_acc.cv_results_)\n",
    "#res1\n",
    "# Report best tuning parameters and score\n",
    "#grid_svc_acc.best_score_\n",
    "#grid_svc_acc.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earlier-publicity",
   "metadata": {},
   "source": [
    "We obtain table representing mean accurracy and std of all the possible set of models. After evaluating the preceding output we are able to choose the best model based on our performance metric. We train final model using the best estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-brand",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model with tuned hyperparameters\n",
    "my_best_SVM_model = SVC(class_weight='balanced',C=0.7, gamma=0.05, kernel='linear', probability=True)\n",
    "my_best_SVM_model.fit(X_train4, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "motivated-sequence",
   "metadata": {},
   "source": [
    "Furthermore, we predict probability of each test set observation with the best model fitted in the training set. We then represent our predictions in a dataframe along icustay_id to identify the patient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "postal-vancouver",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict Probability for each class\n",
    "y_pred_prob_bm = my_best_SVM_model.predict_proba(X_test4)\n",
    "y_pred_prob_bm = pd.DataFrame(y_pred_prob_bm)\n",
    "\n",
    "# Choose column on probability predictions\n",
    "y_pred_prob_bm_k = y_pred_prob_bm.iloc[:,1]\n",
    "\n",
    "# Final data frame with icustay_id identifying patient and corresponding prediction\n",
    "test_predictions_submit_SVM = pd.DataFrame({\"icustay_id\": icustay_id ,\"Hospital_Expire_Flag\": y_pred_prob_bm_k})\n",
    "\n",
    "#Convert to CSV\n",
    "#test_predictions_submit_SVM.to_csv(\"predictionSVM.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civil-borough",
   "metadata": {},
   "source": [
    "Finally, we can now compare our predicted probabilities for the test set patients with the real \"unseen\" outcome of each patient (death or survival)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optional-swedish",
   "metadata": {},
   "source": [
    "## <ins>Extension</ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sticky-biology",
   "metadata": {},
   "source": [
    "One hot encoding was also attempted but was discarded as a reasonable encoding alternative because the resultant model increased dramatically the number of dimensions. Recall the curse of dimensionality reads that as the volume of the space increases, the avaialble data could become sparse. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-philadelphia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # A.3) X_train_cat and X_test_cat\n",
    "# X_train_cat=X_train_cat.drop([\"ICD9_diagnosis\"], axis=1)\n",
    "# X_test_cat=X_test_cat.drop([\"ICD9_diagnosis\"], axis=1)\n",
    "# \n",
    "# X_train_test=pd.concat([X_train_cat,X_test_cat],axis=0)\n",
    "# \n",
    "# # Define function to perform one hot encoding\n",
    "# def getfullitemsforOHE(X_train_test_combined,categorical_columns,sort=False):\n",
    "#     def sortornot(X):\n",
    "#         if sort==False:\n",
    "#             return X\n",
    "#         else:\n",
    "#             return sorted(X)\n",
    "#     fulllist=[]\n",
    "#     for feat in categorical_columns:\n",
    "#         fulllist.append(sortornot(X_train_test_combined[feat].unique()))\n",
    "#     return fulllist\n",
    "# \n",
    "# \n",
    "# categorical_columns = X_train_test.columns.tolist()\n",
    "# cats=getfullitemsforOHE(X_train_test,categorical_columns)\n",
    "# \n",
    "# ohe=OneHotEncoder(categories=cats, sparse=False,handle_unknown=\"ignore\")\n",
    "# \n",
    "# # A.4) Train set one hot encoding\n",
    "# \n",
    "# X_train_cat_transformed=ohe.fit_transform(X_train_cat[categorical_columns])\n",
    "# X_train_cat_transformed=pd.DataFrame(X_train_cat_transformed,columns=ohe.get_feature_names(categorical_columns))\n",
    "# \n",
    "# # A.5) Test set ine hot encoding\n",
    "# \n",
    "# X_test_cat_transformed=ohe.transform(X_test_cat[categorical_columns])\n",
    "# X_test_cat_transformed=pd.DataFrame(X_test_cat_transformed,columns=ohe.get_feature_names(categorical_columns))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
